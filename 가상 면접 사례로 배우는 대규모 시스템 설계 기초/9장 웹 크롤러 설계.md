- 크롤러 사용
	- 검색 엔진 인덱싱
	- 웹 아카이빙
	- 웹 마닝
	- 웹 모니터링
## 1단계 문제 이해 및 설계 범위 확정
### 개략적 규모 추정
- 매달 10억 개의 웹 페이지를 다운
- QPS = 10억 / 30일 / 24시간 / 3600초 = 대략 400 페이지/초
- 최대 QPS = 2 X QPS = 800
- 웹 페이지의 크기 평균은 500k라고 가정
- 10억 페이지 X 500k = 500TB/월
- 500T X 12개월 X 5년 = 30PB

## 2단계 개략적 설계안 제시 및 동의 구하기
### 시작 URL 집합
- 시작 URL 집합은 웹 크롤러가 크롤링을 시작하는 출발점
- 일반적으로는 전체 URL 공간을 작읍 부분집합으로 나누는 전략을 쓴다.

### 미수집 URL 저장소
- 다운로드할 URL을 저장 관리하는 컴포넌트

### HTML 다운로더
- 웹 페이지를 다운로드하는 컴포넌트

### 도메인 이름 변환기
- 웹 페이지를 다운받으려면 URL을 IP 주소로 변환하는 절차가 필요

### 중복 콘텐츠인가?
- 확인하는 효과적인 방법은 웹 페이지의 해시 값을 비교하는 것

### URL 추출기
- HTML 페이지를 파싱하여 링크들을 골라내는 역할

### URL 필터
- URL 필터는 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 젭근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할

### 이미 방문한 URL?
- 서버 부하를 줄이고 시스템이 무한 루프에 빠지는 일을 방지

### URL 저장소
- 이미 방문한 URL을 보관하는 저장소

### 웹 크롤러 작업 흐름
1. 시작 URL들을 미수집 URL 저장소에 저장
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져옴
3. IP 주소로 접소하여 웹 페이지 다운
4. 올바른 형식을 갖춘 페이지 인지 콘텐츠 파서로 검증
5. 중복 콘텐츠 확인
6. URL 추출기로 링크 추출
7. URL 필터로 전달

## 3단계 상세 설계
### DFS를 쓸 것인가, BFS를 쓸 것인가
- 웹은 유향 그래프나 같다.
- 웹 크롤러는 보통 BFS, 즉 너비 우선 탐색법을 사용
- 두 가지 문제점
	- 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아간다
	- 표준적 BFS 알고리즘은 URL 간에 우선순위를 두지 않는다.
### 미수집 URL 저장소
- 미수집 URL 저장소를 활용하면 위의 문제 해결

**예의**
- 웹 크롤러는 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 보내는 것을 삼가야 한다.
- 동일 웹사이트에 대해서는 한 번에 한 페이지만 요청

**우선순위**
- 유용성에 따라 URL의 우선순위를 나눌 때는 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도를 사용할 수 있다.
- 순위결정장치 컴포턴트 사용

**신선도**
- 데이터의 신선함을 유지하기 위해서는 이미 다운로드한 페이지라고 해도 주기적으로 재수집할 필요가 있다.

### HTML 다운로더
**Robots.txt**
- 로봇 제외 프로토콜이라고 부르기도 하는 Robots.txt는 웹사이트가 크롤러와 소통하는 표준적 방법.
- 이 파일에는 크롤러가 수집해도 되는 페이지 목록이 들어 있다.

**성능 최적화**
1. 분산 크롤링
2. 도메인 이름 변환 결과 캐시
3. 지역성
4. 짧은 타임아웃

**안정성**
- 안정 해시
- 크롤링 상태 및 수집 데이터 저장
- 예외 처리
- 데이터 검증

**확장성**
- 새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 신경

### 문제 있는 콘첸츠 감지 회피
**1.중복 컨텐츠**
- 해시나 체크섬을 사용

**2.거미 덫**
- 크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지
- URL의 최대 길이를 제한
- 만능 해결책은 없다

**3.데이터 노이즈**
- 광고나 스크립트 코드, 스팸 URL을 가능하다면 제외

## 4단계 마무리
- 추가 논의 사항
	- 서버 측 렌더링
	- 원치 않는 페이지 필터링
	- 데이터베이스 다중화 및 샤딩
	- 수평적 규모 확장성
	- 가용성, 일관성, 안정성
	- 데이터 분석 솔루션
