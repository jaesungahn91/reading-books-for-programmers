- 크롤러 사용
	- 검색 엔진 인덱싱
	- 웹 아카이빙
	- 웹 마닝
	- 웹 모니터링
## 1단계 문제 이해 및 설계 범위 확정
### 개략적 규모 추정
- 매달 10억 개의 웹 페이지를 다운
- QPS = 10억 / 30일 / 24시간 / 3600초 = 대략 400 페이지/초
- 최대 QPS = 2 X QPS = 800
- 웹 페이지의 크기 평균은 500k라고 가정
- 10억 페이지 X 500k = 500TB/월
- 500T X 12개월 X 5년 = 30PB

## 2단계 개략적 설계안 제시 및 동의 구하기
### 시작 URL 집합
- 시작 URL 집합은 웹 크롤러가 크롤링을 시작하는 출발점
- 일반적으로는 전체 URL 공간을 작읍 부분집합으로 나누는 전략을 쓴다.

### 미수집 URL 저장소
- 다운로드할 URL을 저장 관리하는 컴포넌트

### HTML 다운로더
- 웹 페이지를 다운로드하는 컴포넌트

### 도메인 이름 변환기
- 웹 페이지를 다운받으려면 URL을 IP 주소로 변환하는 절차가 필요

### 중복 콘텐츠인가?
- 확인하는 효과적인 방법은 웹 페이지의 해시 값을 비교하는 것

### URL 추출기
- HTML 페이지를 파싱하여 링크들을 골라내는 역할

### URL 필터
- URL 필터는 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 젭근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할

### 이미 방문한 URL?
- 서버 부하를 줄이고 시스템이 무한 루프에 빠지는 일을 방지

### URL 저장소
- 이미 방문한 URL을 보관하는 저장소

### 웹 크롤러 작업 흐름
1. 시작 URL들을 미수집 URL 저장소에 저장
2. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져옴
3. IP 주소로 접소하여 웹 페이지 다운
4. 올바른 형식을 갖춘 페이지 인지 콘텐츠 파서로 검증
5. 중복 콘텐츠 확인
6. URL 추출기로 링크 추출
7. URL 필터로 전달
8. 
