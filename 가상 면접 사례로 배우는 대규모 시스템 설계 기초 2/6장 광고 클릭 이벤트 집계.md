## 1단계 : 문제 이해 및 설계 범위 확정
### 기능 요구사항
- 지난 M분 동안의 ad_id 클릭 수 집계
- 매분 가장 많이 클릭된 상위 100개 광고 아이디를 반환
- 다양한 속성에 따른 집계 필터링을 지원
- 데이터의 양은 페이스북이나 구글 규모
### 비기능 요구사항
- 집계 결과 정확성은 데이터가 RTB 및 광고 과금에 사용되므로 중요
- 지연되거나 중복된 이벤트를 적절히 처리할 수 있어야 함
- 견고성 : 부분적인 장애는 감내할 수 있어야 함
- 지연 시간 요구사항 : 전체 처리 시간은 최대 수 분을 넘지 않아야 함
### 개략적 추정
- DAU : 10억 명
- 하루에 10억건 클릭 이벤트
- QPS = 10,000
- 최대 QPS = 50,000
- 일일 저장소 요구량 100GB, 월 대략 3TB
## 2단계 : 개략적 설계안 제시 및 동의 구하기
### 질의 API 설계
- 지난 M분 동안 각 ad_id에 발생한 클릭 수 집계
- 지난 M분 동안 가장 많은 클릭이 발생한 상위 N개 ad_id 목록 반환
- 다양한 속성을 기준으로 집계 결과를 필터링하는 기능 지원
### 데이터 모델
- 원시 데이터
- 집계 결과 데이터
### 올바른 데이터베이스의 선택
- 원시 데이터 : 카산드라
- 집계 결과 데이터 : 카산드라
### 개략적 설계안
- 비동기 처리 : 카프카 같은 메시지 큐를 도입
### 집계 서비스
- 맵리듀스 프레임워크 사용
- 맵 노드 : 데이터를 필터링하고 변환
- 집계 노드 : 매분 집계
- 리듀스 노드 : 집계 결과 축약
## 3단계 : 상세 설계
### 스트리밍 vs 일괄 처리
- 스트림 처리 : 집계 결과 생성
- 일괄 처리 : 이력 데이터 백업
- 카파 아키텍처 : 일괄 처리와 스트리밍 처리 경로를 하나로 결합
### 시간
- 타임스탬프로 이벤트 발생 시간 사용
- 늦게 도착한 이벤트를 올바르게 처리하기 위해 워터마크 사용
### 집계 윈도
- 텀블링 윈도사용
	- 시간을 같은 크기의 겹치지 않는 구간으로 분할
### 전달 보장
- 카프라 사용
- 데이터 중복 제거
	- HDFS나 S3 같은 외부 파일 저장소에 오프셋 기록
	- 데이터 손신을 막으려면 다운스트림에서 집계 결과 수신 확인 응답을 받은 후에 오프셋을 저장
- 이벤트를 정확하게 한 번만 처리하기 위해 분산 트랜잭션 사용
### 시스템 규모 확장
- 메시지 큐의 규모 확장
	- 소비자 재조정 메커니즘
	- 브로커
		- 해시키
		- 파티션의 수
		- 토픽의 물리적 샤딩
- 집계 서버 확장
	- 노드의 추가/삭제
- 데이터베이스 확장
	- 카산드라에서 수평 확장을 지원
### 핫스팟 문제
- 더 많은 집계 서비스 노드를 할당하여 완화
- 지원 관리자 사용
### 결함 내성
- 시스템 상태, 집계 데이터 등을 스냅숏으로 저장
### 데이터 모니터링 및 정확성
- 모니터링 지표
	- 지연 시간
	- 메시지 큐 크기
	- 집계 노드의 시스템 자원
- 조정
	- 데이터의 무결성을 보증하는 기법
### 대안적 설계안
- 하이브 + 일래스틱서치 + OLAP 데이터베이스